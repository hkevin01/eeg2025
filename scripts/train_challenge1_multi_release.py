#!/usr/bin/env python3
"""
Challenge 1: Multi-Release Response Time Prediction
===================================================
CRITICAL FIX FOR OVERFITTING: Train on R1-R5 instead of just R5

Problem: Previous model trained only on R5, tested on R12 ‚Üí 10x degradation
Solution: Multi-release training for better generalization

Model improvements:
- Reduced parameters: 800K ‚Üí 200K (less overfitting)
- Increased dropout: 0.3 ‚Üí 0.5 (better regularization)
- Weight decay: 1e-4 (L2 regularization)
- Cross-release validation: Train R1-R4, validate R5
"""
import os
import sys
from pathlib import Path
import time
import warnings
import logging
import traceback
from datetime import datetime
import json

warnings.filterwarnings('ignore')

# Setup comprehensive logging
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)
crash_log_file = log_dir / f"challenge1_crash_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(crash_log_file),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Force CPU
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['HIP_VISIBLE_DEVICES'] = ''

logger.info("="*80)
logger.info("üöÄ Challenge 1 Training Started")
logger.info(f"Crash log: {crash_log_file}")
logger.info("="*80)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, ConcatDataset
import numpy as np
from braindecode.preprocessing import Preprocessor, preprocess
from braindecode.preprocessing import create_fixed_length_windows
from eegdash import EEGChallengeDataset
from eegdash.hbn.windows import (
    annotate_trials_with_target,
    add_aux_anchors,
)

print("="*80)
print("üéØ CHALLENGE 1: MULTI-RELEASE RESPONSE TIME PREDICTION")
print("="*80)
print("Training on: R1, R2, R3, R4")
print("Validation on: R5")
print("Expected improvement: 10x better generalization")
print("="*80)

EPOCH_LEN_S = 2.0
SHIFT_AFTER_STIM = 0.5


class MultiReleaseDataset(Dataset):
    """Dataset combining multiple releases for better generalization"""

    def __init__(self, releases, mini=True, cache_dir='data/raw'):
        """
        Args:
            releases: List of release names ['R1', 'R2', ...]
            mini: Use mini dataset (True for testing, False for full)
            cache_dir: Cache directory for data
        """
        self.releases = releases
        self.windows_datasets = []
        self.release_labels = []

        print(f"\nüìÇ Loading releases: {releases}")
        logger.info(f"Loading {len(releases)} releases: {releases}")

        for release_idx, release in enumerate(releases, 1):
            print(f"\n  [{release_idx}/{len(releases)}] Loading {release}...")
            logger.info(f"Loading release {release} ({release_idx}/{len(releases)})")
            release_start = time.time()

            # Load dataset
            try:
                dataset = EEGChallengeDataset(
                    release=release,
                    mini=mini,
                    query=dict(task="contrastChangeDetection"),
                    cache_dir=Path(cache_dir)
                )
                print(f"    Datasets: {len(dataset.datasets)}")
                logger.info(f"  {release}: Loaded {len(dataset.datasets)} datasets")
            except Exception as e:
                logger.error(f"  ‚ùå Failed to load {release}: {e}")
                raise

            # Filter out corrupted datasets by testing each one
            print(f"    Checking for corrupted files in {len(dataset.datasets)} datasets...")
            logger.info(f"  {release}: Checking {len(dataset.datasets)} files for corruption...")
            valid_datasets = []
            corrupted_count = 0

            for idx, ds in enumerate(dataset.datasets):
                try:
                    # Try to access the raw data to check if file is corrupted
                    _ = ds.raw.n_times
                    valid_datasets.append(ds)

                    # Progress indicator every 50 files
                    if (idx + 1) % 50 == 0:
                        print(f"    Checked {idx + 1}/{len(dataset.datasets)} files... ({len(valid_datasets)} valid)")
                        logger.info(f"  {release}: Progress {idx + 1}/{len(dataset.datasets)} - {len(valid_datasets)} valid, {corrupted_count} corrupted")

                except (IndexError, ValueError, OSError, RuntimeError) as e:
                    corrupted_count += 1
                    error_msg = str(e)[:100]
                    print(f"    ‚ö†Ô∏è  Skipping corrupted dataset {idx}: {error_msg}")
                    logger.warning(f"  {release}: Corrupted file {idx}: {error_msg}")
                    continue

            # Create a new dataset with only valid datasets
            from braindecode.datasets import BaseConcatDataset
            dataset = BaseConcatDataset(valid_datasets)

            release_time = time.time() - release_start
            skipped = corrupted_count
            print(f"    ‚úÖ Valid datasets: {len(dataset.datasets)} (skipped {skipped} corrupted) - {release_time:.1f}s")
            logger.info(f"  {release}: Complete - {len(dataset.datasets)} valid, {skipped} corrupted, {release_time:.1f}s")

            # Preprocess
            print(f"    Preprocessing {len(dataset.datasets)} datasets...")
            logger.info(f"  {release}: Starting preprocessing...")
            preprocessors = [
                Preprocessor(
                    annotate_trials_with_target,
                    apply_on_array=False,
                    target_field="rt_from_stimulus",
                    epoch_length=EPOCH_LEN_S,
                    require_stimulus=True,
                    require_response=True,
                ),
                Preprocessor(add_aux_anchors, apply_on_array=False),
            ]

            preprocess(dataset, preprocessors, n_jobs=-1)

            # Create windows
            windows_dataset = create_fixed_length_windows(
                dataset,
                start_offset_samples=int(SHIFT_AFTER_STIM * 100),  # 100 Hz
                stop_offset_samples=None,
                window_size_samples=int(EPOCH_LEN_S * 100),
                window_stride_samples=int(EPOCH_LEN_S * 100),
                drop_last_window=False,
                mapping=None,
                preload=False,
                picks=None,
                reject=None,
                flat=None,
                targets_from="metadata",
                last_target_only=True,
                on_missing="ignore",
            )

            print(f"    Windows: {len(windows_dataset)}")

            self.windows_datasets.append(windows_dataset)
            self.release_labels.extend([release] * len(windows_dataset))

        # Concatenate all releases
        self.combined_dataset = ConcatDataset(self.windows_datasets)
        print(f"\n‚úÖ Total windows: {len(self.combined_dataset)}")
        print(f"   Releases: {set(self.release_labels)}")

    def __len__(self):
        return len(self.combined_dataset)

    def __getitem__(self, idx):
        windows_ds, rel_idx = self._get_dataset_and_index(idx)
        X, y, _ = windows_ds[rel_idx]

        # X shape: (1, n_channels, n_timepoints) or (n_channels, n_timepoints)
        if X.ndim == 3:
            X = X.squeeze(0)

        # Normalize
        X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-8)

        return torch.FloatTensor(X), torch.FloatTensor([y])

    def _get_dataset_and_index(self, idx):
        """Find which dataset and local index for given global index"""
        if idx < 0:
            idx = len(self) + idx

        dataset_idx = 0
        while idx >= len(self.windows_datasets[dataset_idx]):
            idx -= len(self.windows_datasets[dataset_idx])
            dataset_idx += 1

        return self.windows_datasets[dataset_idx], idx


class CompactResponseTimeCNN(nn.Module):
    """Smaller CNN to reduce overfitting (200K params vs 800K)"""

    def __init__(self):
        super().__init__()

        self.features = nn.Sequential(
            # Conv1: channels x 200 -> 32x100
            nn.Conv1d(129, 32, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.3),

            # Conv2: 32x100 -> 64x50
            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.4),

            # Conv3: 64x50 -> 128x25
            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.5),

            # Global pooling
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten()
        )

        self.regressor = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        features = self.features(x)
        output = self.regressor(features)
        return output


def compute_nrmse(y_true, y_pred):
    """Normalized RMSE"""
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)
    std = np.std(y_true)
    return rmse / std if std > 0 else 0.0


def train_model(model, train_loader, val_loader, epochs=50):
    """Train with strong regularization"""

    print("\n" + "="*80)
    print("üî• Training Multi-Release Model")
    print("="*80)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Parameters: {total_params:,}")

    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    best_nrmse = float('inf')
    patience_counter = 0
    patience = 15

    for epoch in range(epochs):
        print(f"\n{'='*80}")
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"{'='*80}")

        # Train
        model.train()
        train_loss = 0
        train_preds = []
        train_labels = []

        for data, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_loss += loss.item()
            train_preds.extend(outputs.detach().numpy().flatten())
            train_labels.extend(labels.numpy().flatten())

        # Validate
        model.eval()
        val_loss = 0
        val_preds = []
        val_labels = []

        with torch.no_grad():
            for data, labels in val_loader:
                outputs = model(data)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                val_preds.extend(outputs.numpy().flatten())
                val_labels.extend(labels.numpy().flatten())

        # Metrics
        train_nrmse = compute_nrmse(np.array(train_labels), np.array(train_preds))
        val_nrmse = compute_nrmse(np.array(val_labels), np.array(val_preds))

        print(f"Train NRMSE: {train_nrmse:.4f}")
        print(f"Val NRMSE:   {val_nrmse:.4f}")

        scheduler.step()

        # Early stopping
        if val_nrmse < best_nrmse:
            best_nrmse = val_nrmse
            patience_counter = 0
            torch.save(model.state_dict(), 'weights_challenge_1_multi_release.pt')
            print(f"‚úÖ Best model saved (NRMSE: {best_nrmse:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\n‚èπÔ∏è  Early stopping (no improvement for {patience} epochs)")
                break

    return best_nrmse


def main():
    start_time = time.time()

    # Load training releases (R1-R4)
    print("\nüì¶ Loading training data (R1-R4)...")
    train_dataset = MultiReleaseDataset(
        releases=['R1', 'R2', 'R3', 'R4'],
        mini=False,  # FULL DATASET for production training
        cache_dir='data/raw'
    )

    # Load validation release (R5)
    print("\nüì¶ Loading validation data (R5)...")
    val_dataset = MultiReleaseDataset(
        releases=['R5'],
        mini=False,  # FULL DATASET for validation
        cache_dir='data/raw'
    )

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

    print(f"\nüìä Dataset sizes:")
    print(f"   Train: {len(train_dataset)} windows")
    print(f"   Val:   {len(val_dataset)} windows")

    # Create model
    model = CompactResponseTimeCNN()

    # Train
    best_nrmse = train_model(model, train_loader, val_loader, epochs=50)

    # Summary
    elapsed = time.time() - start_time
    print("\n" + "="*80)
    print("‚úÖ TRAINING COMPLETE")
    print("="*80)
    print(f"Best validation NRMSE: {best_nrmse:.4f}")
    print(f"Target: < 0.5 (competitive)")
    print(f"Previous (R5 only): 0.47 validation ‚Üí 4.05 test (10x degradation)")
    print(f"Expected (R1-R5): ~0.70 validation ‚Üí ~1.4 test (2x better!)")
    print(f"Time: {elapsed/60:.1f} minutes")
    print(f"Model saved: weights_challenge_1_multi_release.pt")
    print("="*80)

    logger.info("‚úÖ Training completed successfully!")
    return best_nrmse


if __name__ == "__main__":
    try:
        logger.info("Starting main training pipeline...")
        result = main()
        logger.info(f"‚úÖ SUCCESS! Best NRMSE: {result:.4f}")
        sys.exit(0)
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è  Training interrupted by user (Ctrl+C)")
        sys.exit(130)
    except Exception as e:
        logger.error("="*80)
        logger.error("üí• FATAL ERROR - Training crashed!")
        logger.error("="*80)
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error message: {str(e)}")
        logger.error("\nFull traceback:")
        logger.error(traceback.format_exc())
        logger.error("="*80)
        logger.error(f"Crash details saved to: {crash_log_file}")
        logger.error("="*80)
        sys.exit(1)
