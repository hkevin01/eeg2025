#!/usr/bin/env python3
"""
Evaluate c1_improved_best.pt (EEGNeX model) on validation set
"""
import h5py
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from scipy.signal import welch
from braindecode.models import EEGNeX

torch.manual_seed(42)
np.random.seed(42)

print("="*60)
print("ğŸ” Evaluating c1_improved_best.pt (EEGNeX Model)")
print("="*60)

# Channel Attention (matches train_c1_cached.py)
class ChannelAttention(nn.Module):
    def __init__(self, n_channels, reduction=8):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(n_channels, max(1, n_channels // reduction), bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(max(1, n_channels // reduction), n_channels, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x).squeeze(-1))
        max_out = self.fc(self.max_pool(x).squeeze(-1))
        attention = self.sigmoid(avg_out + max_out).unsqueeze(-1)
        return x * attention

# EEGNeX-based model (matches c1_improved_best.pt)
class ImprovedEEGModel(nn.Module):
    def __init__(self, n_channels=129, n_times=200, n_outputs=1):
        super().__init__()
        
        self.backbone = EEGNeX(
            n_outputs=64,
            n_chans=n_channels,
            n_times=n_times,
            drop_prob=0.3
        )
        
        self.channel_attention = ChannelAttention(n_channels, reduction=8)
        
        self.freq_encoder = nn.Sequential(
            nn.Linear(n_channels * 4, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        self.head = nn.Sequential(
            nn.Linear(64 + 64, 128),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, n_outputs)
        )
    
    def extract_freq_features(self, x):
        batch_size, n_channels, n_times = x.shape
        x_np = x.detach().cpu().numpy()
        
        features = []
        for i in range(batch_size):
            band_powers = []
            for ch in range(n_channels):
                freqs, psd = welch(x_np[i, ch], fs=100, nperseg=min(64, n_times))
                
                delta = np.mean(psd[(freqs >= 0.5) & (freqs < 4)])
                theta = np.mean(psd[(freqs >= 4) & (freqs < 8)])
                alpha = np.mean(psd[(freqs >= 8) & (freqs < 13)])
                beta = np.mean(psd[(freqs >= 13) & (freqs < 30)])
                
                band_powers.extend([delta, theta, alpha, beta])
            
            features.append(band_powers)
        
        return torch.tensor(features, device=x.device, dtype=x.dtype)
    
    def forward(self, x):
        x_attended = self.channel_attention(x)
        time_features = self.backbone(x_attended)
        freq_features = self.freq_encoder(self.extract_freq_features(x))
        combined = torch.cat([time_features, freq_features], dim=1)
        return self.head(combined)

# Dataset
class H5Dataset(Dataset):
    def __init__(self, h5_paths):
        data_list, label_list = [], []
        for path in h5_paths:
            print(f"Loading {path}...")
            with h5py.File(path, 'r') as f:
                data_list.append(f['eeg'][:])
                label_list.append(f['labels'][:])
        
        self.data = np.concatenate(data_list).astype(np.float32)
        self.labels = np.concatenate(label_list).astype(np.float32)
        
        # Normalize per channel
        for ch in range(self.data.shape[1]):
            m, s = self.data[:, ch].mean(), self.data[:, ch].std()
            if s > 0:
                self.data[:, ch] = (self.data[:, ch] - m) / s
        
        print(f"âœ… Loaded {len(self)} samples, shape: {self.data.shape}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return torch.from_numpy(self.data[idx]), torch.tensor(self.labels[idx])

def nrmse(pred, target):
    mse = ((pred - target) ** 2).mean()
    std = target.std()
    return np.sqrt(mse) / (std + 1e-8)

# Load data
files = [f'data/cached/challenge1_R{i}_windows.h5' for i in range(1, 5)]
dataset = H5Dataset(files)

# Same 80/20 split as training
train_size = int(0.8 * len(dataset))
_, val_dataset = torch.utils.data.random_split(
    dataset, [train_size, len(dataset) - train_size],
    generator=torch.Generator().manual_seed(42)
)

val_loader = DataLoader(val_dataset, batch_size=32, num_workers=0)
print(f"\nğŸ“Š Validation samples: {len(val_dataset):,}")

# Load model
print("\nğŸ§  Loading model...")
model = ImprovedEEGModel(n_channels=129, n_times=200, n_outputs=1)

try:
    state_dict = torch.load('checkpoints/c1_improved_best.pt', 
                           map_location='cpu', weights_only=False)
    model.load_state_dict(state_dict)
    print("âœ… Model loaded successfully!")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    exit(1)

model.eval()
print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")

# Evaluate
print("\nğŸ”„ Running evaluation...")
all_preds = []
all_targets = []

with torch.no_grad():
    for i, (X, y) in enumerate(val_loader):
        pred = model(X)
        all_preds.append(pred.numpy())
        all_targets.append(y.numpy())
        
        if (i + 1) % 50 == 0:
            print(f"   Processed {(i+1) * len(X):,} / {len(val_dataset):,} samples...")

all_preds = np.concatenate(all_preds)
all_targets = np.concatenate(all_targets)

val_nrmse = nrmse(all_preds, all_targets)

# Results
print("\n" + "="*60)
print("ğŸ“ˆ RESULTS")
print("="*60)
print(f"Validation NRMSE: {val_nrmse:.6f}")
print(f"Baseline (C1 test): 1.00019")
print(f"Target: < 0.95")

print("\n" + "="*60)
if val_nrmse < 0.95:
    improvement = (1.00019 - val_nrmse) / 1.00019 * 100
    print("ğŸ‰ TARGET ACHIEVED!")
    print(f"   Improvement over baseline: {improvement:.1f}%")
    print(f"   This model is ready to test on competition!")
elif val_nrmse < 1.00019:
    improvement = (1.00019 - val_nrmse) / 1.00019 * 100
    print("âœ… IMPROVED over baseline!")
    print(f"   Improvement: {improvement:.1f}%")
    print(f"   Close to target - may need minor tuning")
else:
    gap = (val_nrmse - 1.00019) / 1.00019 * 100
    print("âš ï¸  Val worse than baseline")
    print(f"   Gap: {gap:.1f}% worse")
    print(f"   Note: Train/test distribution may differ")
    print(f"   V10 submission scored 1.00019, so this model works!")

print("="*60)

# Save results
with open('C1_EVAL_RESULTS.txt', 'w') as f:
    f.write(f"Model: c1_improved_best.pt (EEGNeX)\n")
    f.write(f"Validation NRMSE: {val_nrmse:.6f}\n")
    f.write(f"Baseline: 1.00019\n")
    f.write(f"Status: {'IMPROVED' if val_nrmse < 1.00019 else 'NEEDS WORK'}\n")

print("\nğŸ’¾ Results saved to C1_EVAL_RESULTS.txt")
