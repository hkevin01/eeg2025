# ðŸ“‹ EEG2025 Competition - Complete Rules & Guidelines
**Competition:** NeurIPS 2025 EEG Foundation Challenge  
**Source:** https://eeg2025.github.io/rules/  
**Last Updated:** October 17, 2025

---

## ðŸŽ¯ COMPETITION STRUCTURE

### Challenge Format
```
Type:        Regression tasks (supervised learning)
Challenges:  2 separate challenges with combined scoring
Platform:    Codabench (https://www.codabench.org/competitions/4287/)
Timeline:    June 2025 - November 2, 2025
Venue:       NeurIPS 2025 Competition Track
```

### Challenge 1: Cross-Task Transfer Learning
```
Objective:   Predict response time from EEG signals
Task:        Contrast Change Detection (CCD)
Input:       EEG data (129 channels, 100Hz, variable length)
Output:      Response time (continuous value, seconds)
Metric:      NRMSE (Normalized Root Mean Square Error)
Weight:      30% of overall competition score
Pretraining: Suggested to use passive tasks (RS, SuS, MW)
```

### Challenge 2: Externalizing Factor Prediction
```
Objective:   Predict externalizing behavior factor
Task:        Subject-invariant representation learning
Input:       Resting State EEG (129 channels, 100Hz)
Output:      Externalizing factor score (continuous value)
Metric:      NRMSE (Normalized Root Mean Square Error)
Weight:      70% of overall competition score
Note:        Originally included 4 factors, reduced to 1 (externalizing)
```

---

## ðŸ“Š EVALUATION METRICS

### NRMSE (Normalized Root Mean Square Error)
```python
NRMSE = RMSE / std(y_true)

Where:
â”œâ”€ RMSE = sqrt(mean((y_pred - y_true)^2))
â”œâ”€ std(y_true) = standard deviation of ground truth values
â”œâ”€ Lower is better (perfect score = 0.0)
â””â”€ Baseline: Using mean prediction (NRMSE â‰ˆ 1.0)

Properties:
â”œâ”€ Scale-independent (normalized by std)
â”œâ”€ Interpretable: NRMSE = 1.0 means same error as naive baseline
â”œâ”€ NRMSE < 1.0 means better than baseline
â””â”€ NRMSE > 1.0 means worse than baseline
```

### Overall Competition Score
```python
Overall_Score = 0.30 Ã— NRMSE_Challenge1 + 0.70 Ã— NRMSE_Challenge2

Rationale for weights:
â”œâ”€ Challenge 2 (70%): More data, more general, clinical importance
â”œâ”€ Challenge 1 (30%): Smaller dataset, more specific task
â””â”€ Both challenges must perform well to win
```

### Ranking
```
Primary:    Overall score (lower is better)
Tiebreaker: Earlier submission timestamp
Leaderboard: Public leaderboard updated automatically
Final:       Private test set (R12) used for final ranking
```

---

## ðŸ“¦ SUBMISSION REQUIREMENTS

### Required Files
```
submission.zip must contain:
â”œâ”€ submission.py          # Main submission script (REQUIRED)
â”œâ”€ weights_*.pt           # Model weights (any number, any names)
â”œâ”€ methods.pdf            # 2-page methods document (OPTIONAL)
â””â”€ Any additional files   # Libraries, configs, etc. (optional)

Size Limits:
â”œâ”€ Total submission: < 2 GB (recommended < 500 MB)
â”œâ”€ Individual files: No specific limit
â””â”€ Execution time: 24 hours max per submission
```

### submission.py Requirements
```python
# Must contain these two functions:

def challenge1(X):
    """
    Predict response times for Challenge 1.
    
    Args:
        X: List of EEG arrays, each shape (n_channels, n_timepoints)
           - n_channels = 129
           - n_timepoints varies per trial
    
    Returns:
        predictions: np.ndarray of shape (n_trials,)
                    Response times in seconds
    """
    pass

def challenge2(X):
    """
    Predict externalizing factors for Challenge 2.
    
    Args:
        X: List of EEG arrays, each shape (n_channels, n_timepoints)
           - n_channels = 129
           - n_timepoints = fixed length windows
    
    Returns:
        predictions: np.ndarray of shape (n_samples,)
                    Externalizing factor scores
    """
    pass

# Important:
â”œâ”€ Functions must handle variable-length inputs
â”œâ”€ Must work in Codabench execution environment
â”œâ”€ Can import standard libraries (numpy, torch, sklearn, etc.)
â”œâ”€ Can load weights from ./weights/ or specified path
â””â”€ Must return numpy arrays with correct shape
```

### Path Resolution
```python
# Recommended approach for loading weights:
from pathlib import Path

def resolve_path(name="model_file_name"):
    """Handle different execution environments"""
    if Path(f"/app/input/res/{name}").exists():
        return f"/app/input/res/{name}"
    elif Path(f"/app/input/{name}").exists():
        return f"/app/input/{name}"
    elif Path(f"{name}").exists():
        return f"{name}"
    elif Path(__file__).parent.joinpath(f"{name}").exists():
        return str(Path(__file__).parent.joinpath(f"{name}"))
    else:
        raise FileNotFoundError(f"Could not find {name}")

# Usage:
weights_path = resolve_path("weights_challenge_1.pt")
model.load_state_dict(torch.load(weights_path))
```

---

## ðŸ—“ï¸ COMPETITION TIMELINE

### Key Dates
```
Phase 1: Development Phase
â”œâ”€ Start:       June 2025
â”œâ”€ Data Release: Training data (R1-R5) available
â”œâ”€ Validation:  Participants can validate locally
â””â”€ Submissions: Unlimited submissions during this phase

Phase 2: Evaluation Phase
â”œâ”€ Start:       September 2025
â”œâ”€ Test Data:   R12 (hidden test set) used for evaluation
â”œâ”€ Submissions: Limited submissions (check platform)
â””â”€ Leaderboard: Public leaderboard available

Phase 3: Final Evaluation
â”œâ”€ Deadline:    November 2, 2025 (23:59 UTC)
â”œâ”€ Freeze:      Submissions close
â”œâ”€ Evaluation:  Final private test set evaluation
â””â”€ Results:     Announced at NeurIPS 2025

Phase 4: Awards & Presentation
â”œâ”€ NeurIPS:     December 2025 (exact dates TBD)
â”œâ”€ Winners:     Top teams present at NeurIPS
â”œâ”€ Awards:      Cash prizes + travel support
â””â”€ Publication: Competition report paper
```

---

## ðŸ“š DATA USAGE RULES

### Allowed Data
```
âœ… HBN-EEG dataset (provided by competition)
   â”œâ”€ Training: R1, R2, R3, R4, R5
   â”œâ”€ Test: R12 (hidden)
   â””â”€ All tasks: CCD, RS, SuS, MW, SL, SyS

âœ… External EEG datasets (for pretraining)
   â”œâ”€ Publicly available EEG datasets
   â”œâ”€ Must be documented in methods
   â””â”€ Must not include HBN participants

âœ… Pretrained models
   â”œâ”€ Foundation models (if publicly available)
   â”œâ”€ Self-supervised pretraining
   â””â”€ Transfer learning from other domains
```

### Prohibited Data
```
âŒ Test set data (R12) - obviously prohibited
âŒ HBN participants' data outside competition dataset
âŒ Private datasets not publicly available
âŒ Manual labeling of test data
âŒ Leaking information from test set
```

### Data Splits
```
Training Strategy (Recommended):
â”œâ”€ Use R1-R5 for training/validation
â”œâ”€ Implement cross-validation across releases
â”œâ”€ Test generalization across different releases
â””â”€ Avoid overfitting to specific release distribution

Competition Evaluation:
â”œâ”€ Models evaluated on R12 (hidden test set)
â”œâ”€ R12 contains data from different subjects
â”œâ”€ Distribution may differ from R1-R5
â””â”€ Generalization is key to success
```

---

## ðŸ‘¥ TEAM RULES

### Team Composition
```
âœ… Individual participants allowed
âœ… Teams allowed (max size: check platform)
âœ… Team mergers: Allowed before deadline
âœ… Multiple teams: One person can be in multiple teams
```

### Code Sharing
```
âœ… Share code/ideas in forums (encouraged)
âœ… Open source submissions after competition (encouraged)
âœ… Collaborate with others
âš ï¸  Must clearly attribute external code/methods
âŒ Plagiarism not allowed
```

### Submission Rules
```
âœ… Multiple submissions allowed during development
âœ… Select best submission before deadline
âš ï¸  Limited submissions in final evaluation period
âŒ No submissions after deadline
```

---

## ðŸ† AWARDS & PRIZES

### Prize Categories
```
1st Place:
â”œâ”€ Cash prize: Check competition website
â”œâ”€ NeurIPS presentation opportunity
â”œâ”€ Travel support to NeurIPS 2025
â””â”€ Co-authorship on competition paper

2nd Place:
â”œâ”€ Cash prize: Check competition website
â”œâ”€ NeurIPS poster presentation
â””â”€ Co-authorship on competition paper

3rd Place:
â”œâ”€ Cash prize: Check competition website
â””â”€ Recognition in competition paper

Special Awards:
â”œâ”€ Diversity & Inclusion Award
â”œâ”€ Best Novel Method Award
â”œâ”€ Most Efficient Model Award
â””â”€ Check website for details
```

### Winner Requirements
```
Mandatory for Prize Winners:
â”œâ”€ [ ] Valid submission by deadline
â”œâ”€ [ ] Reproducible code
â”œâ”€ [ ] Detailed methods document (2-page+)
â”œâ”€ [ ] Code release (open source preferred)
â”œâ”€ [ ] Fact sheet / model card
â””â”€ [ ] Presentation at NeurIPS 2025 (top teams)

Timeline:
â”œâ”€ Winners announced: After competition close
â”œâ”€ Code review: Within 2 weeks of announcement
â”œâ”€ Methods paper: Within 1 month
â””â”€ NeurIPS presentation: December 2025
```

---

## ðŸ“ METHODS DOCUMENTATION

### Required for Winners
```
Methods Document Must Include:
â”œâ”€ Model architecture description
â”œâ”€ Training procedure
â”œâ”€ Hyperparameters
â”œâ”€ Data preprocessing
â”œâ”€ Augmentation strategies
â”œâ”€ External data used (if any)
â”œâ”€ Computational requirements
â”œâ”€ Code repository link
â””â”€ Reproducibility instructions

Format:
â”œâ”€ PDF format
â”œâ”€ 2 pages minimum (no maximum for winners)
â”œâ”€ LaTeX or similar professional format
â”œâ”€ Figures and tables allowed
â””â”€ References to prior work
```

### Recommended Content
```
1. Introduction
   â””â”€ Brief overview of approach

2. Architecture
   â”œâ”€ Model diagrams
   â”œâ”€ Parameter counts
   â””â”€ Design rationale

3. Training
   â”œâ”€ Optimization details
   â”œâ”€ Data splits
   â”œâ”€ Augmentation
   â””â”€ Hyperparameters

4. Results
   â”œâ”€ Validation scores
   â”œâ”€ Ablation studies
   â””â”€ Error analysis

5. Discussion
   â”œâ”€ What worked
   â”œâ”€ What didn't work
   â””â”€ Future directions
```

---

## âš–ï¸ FAIR PLAY & ETHICS

### Code of Conduct
```
âœ… Respect other participants
âœ… Follow competition rules
âœ… Report bugs/issues to organizers
âœ… Help improve competition
âš ï¸  Use computational resources responsibly
âŒ No cheating (manual labeling, test set leakage)
âŒ No harassment or discrimination
âŒ No attempts to hack/exploit platform
```

### Intellectual Property
```
Your Submission:
â”œâ”€ You retain all rights to your code
â”œâ”€ You grant organizers right to evaluate
â”œâ”€ You decide on open source vs proprietary
â””â”€ Winners encouraged to open source

Competition Data:
â”œâ”€ HBN dataset: Follow HBN data usage agreement
â”œâ”€ Competition format: Free to use in publications
â”œâ”€ Starter code: MIT License (free to use/modify)
â””â”€ Credit organizers if using competition setup
```

### Privacy & Data Protection
```
âœ… Anonymized EEG data (no personal identifiers)
âœ… Follow HBN data usage policies
âœ… Do not attempt to re-identify subjects
âš ï¸  Computational environment is sandboxed
âŒ No data extraction from test set
```

---

## ðŸ”§ TECHNICAL REQUIREMENTS

### Execution Environment
```
Platform: Codabench
â”œâ”€ OS: Linux
â”œâ”€ Python: 3.9+
â”œâ”€ Libraries: NumPy, PyTorch, TensorFlow, scikit-learn
â”œâ”€ GPU: May or may not be available
â””â”€ Memory: Limited (design accordingly)

Recommendations:
â”œâ”€ Test on CPU (GPU not guaranteed)
â”œâ”€ Optimize memory usage
â”œâ”€ Handle large datasets efficiently
â””â”€ Use checkpointing for large models
```

### Dependencies
```
Allowed:
âœ… Standard Python libraries
âœ… PyTorch, TensorFlow, JAX
âœ… scikit-learn, pandas, NumPy
âœ… MNE-Python (EEG processing)
âœ… Any pip-installable package

Installation:
â”œâ”€ Include requirements.txt
â”œâ”€ Or use Docker (if supported)
â””â”€ Organizers install dependencies

Restrictions:
âš ï¸  No internet access during evaluation
âš ï¸  No external API calls
âŒ No proprietary/licensed software
```

---

## ï¿½ï¿½ SUPPORT & RESOURCES

### Getting Help
```
Discord:    https://discord.gg/8jd7nVKwsc
Forum:      Competition platform forum
Email:      Check competition website
FAQ:        https://eeg2025.github.io/faq/
```

### Resources
```
Starter Kit:    https://eeg2025.github.io/baseline/
GitHub:         https://github.com/eeg2025/startkit
Documentation:  https://eeg2025.github.io/data/
Paper:          https://arxiv.org/abs/2506.19141
```

---

## âœ… PRE-SUBMISSION CHECKLIST

### Before Submitting
```
Code:
â”œâ”€ [ ] submission.py implements challenge1() and challenge2()
â”œâ”€ [ ] Functions return correct array shapes
â”œâ”€ [ ] Model weights load correctly
â”œâ”€ [ ] Tested locally on sample data
â”œâ”€ [ ] No hardcoded paths (use resolve_path())
â”œâ”€ [ ] No dependencies on local files
â””â”€ [ ] Code is reproducible

Package:
â”œâ”€ [ ] All files in .zip archive
â”œâ”€ [ ] File sizes reasonable (< 500 MB)
â”œâ”€ [ ] No corrupted files
â”œâ”€ [ ] Test unzip works correctly
â””â”€ [ ] Folder structure correct

Documentation:
â”œâ”€ [ ] Methods document included (optional but recommended)
â”œâ”€ [ ] README with instructions (optional)
â”œâ”€ [ ] requirements.txt for dependencies
â””â”€ [ ] Code comments for clarity

Testing:
â”œâ”€ [ ] Ran locally without errors
â”œâ”€ [ ] Validated output format
â”œâ”€ [ ] Checked NRMSE on validation set
â””â”€ [ ] Ready for Codabench evaluation
```

---

**Document Created:** October 17, 2025  
**Competition URL:** https://eeg2025.github.io/  
**Codabench:** https://www.codabench.org/competitions/4287/  
**Deadline:** November 2, 2025, 23:59 UTC  
**Status:** Competition Active - 16 days remaining

ðŸ† **Good luck and may the best model win!**
