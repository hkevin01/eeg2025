# ROCm SDK Complete Setup - October 25, 2025

## ‚úÖ SUMMARY

All GPU training issues have been resolved by properly configuring the ROCm SDK environment and fixing compatibility issues.

## üéØ FIXES APPLIED

###  1. ROCm SDK as System Default

**File Created**: `scripts/setup/setup_rocm_system.sh`

**Changes to ~/.bashrc** (Permanent):
```bash
# ROCm SDK Environment
export ROCM_PATH="/opt/rocm_sdk_612"
export LD_LIBRARY_PATH="/opt/rocm_sdk_612/lib:/opt/rocm_sdk_612/lib64:/opt/rocm_sdk_612/lib/python3.11/site-packages/torch/lib:$LD_LIBRARY_PATH"
export PYTHONPATH="/opt/rocm_sdk_612/lib/python3.11/site-packages:$PYTHONPATH"
export PATH="/opt/rocm_sdk_612/bin:$PATH"

# ROCm GPU configuration
export HIP_VISIBLE_DEVICES=0
export HSA_OVERRIDE_GFX_VERSION=10.3.0
export PYTORCH_ROCM_ARCH="gfx1010"

# Memory optimization
export HSA_XNACK=0
export HSA_FORCE_FINE_GRAIN_PCIE=1
export ROCM_MALLOC_PREFILL=1

# MNE Configuration (prevent GUI issues)
export MNE_USE_CUDA=false
export QT_QPA_PLATFORM=offscreen
export MPLBACKEND=Agg

# Aliases
alias python3='/opt/rocm_sdk_612/bin/python3'
alias python='/opt/rocm_sdk_612/bin/python3'
alias pip3='/opt/rocm_sdk_612/bin/pip3'
alias pip='/opt/rocm_sdk_612/bin/pip3'
```

### 2. Memory Rule Documentation

**File**: `.github/instructions/rocm_sdk_rule.instruction.md`

Permanent memory instruction to always use ROCm SDK for GPU training.

### 3. Training Script Fixes

**Files Modified**:

- `scripts/launchers/start_c1_enhanced_training.sh` - Uses ROCm SDK Python, complete lib paths
- `train_c1_enhanced.py` - Fixed DataLoader (num_workers=0), batch_size=4

**Key Changes**:
```bash
# Training script now uses
/opt/rocm_sdk_612/bin/python3 train_c1_enhanced.py \
    --batch_size 4 \  # Reduced from 16
    --mixup_alpha 0.0  # Disabled to avoid HIP kernel errors
```

```python
# DataLoader configuration
DataLoader(..., num_workers=0, ...)  # Changed from 4 to 0
```

### 4. ROCm Compatibility Issues Resolved

**Issue**: `RuntimeError: HIP error: invalid device function` during mixup
**Solution**: Disabled mixup (set mixup_alpha=0.0)
**Impact**: Training still has:
- ‚úÖ Temporal Attention (4-head)
- ‚úÖ MultiScaleFeaturesExtractor (3 branches)
- ‚úÖ Temporal Masking (15%)
- ‚úÖ Magnitude Warping (30%)
- ‚úÖ SAM Optimizer
- ‚ùå Mixup (disabled due to ROCm HIP kernel issue)

## üìä CURRENT TRAINING STATUS

**Experiment**: `enhanced_v4_rocm_nomixup`
**Model**: EnhancedEEGNeX (254,529 parameters)
**Status**: Running in background
**Log**: `training_c1_enhanced.log` or `nohup.out`

**Configuration**:
- Batch size: 4
- Epochs: 30
- Learning rate: 0.001
- SAM rho: 0.05
- Device: AMD Radeon RX 5600 XT (6.43 GB)
- Data: 100 subjects (50 per dataset)

## ‚úÖ VERIFIED WORKING

```bash
‚úÖ Python: 3.11.11 (ROCm SDK)
‚úÖ PyTorch: 2.4.1
‚úÖ ROCm/HIP: 6.1.40093-e774eb382
‚úÖ GPU: AMD Radeon RX 5600 XT
‚úÖ VRAM: 5.98 GB
‚úÖ MNE: 1.10.2
‚úÖ braindecode: 1.2.0
‚úÖ pandas: 2.3.3
‚úÖ numpy: 2.2.6
```

## üîç MONITORING

**Check training progress**:
```bash
tail -f training_c1_enhanced.log
# or
tail -f nohup.out
```

**Check GPU usage**:
```bash
rocm-smi
watch -n 1 'rocm-smi'
```

**Check process**:
```bash
ps aux | grep train_c1_enhanced
```

## ‚è±Ô∏è ESTIMATED TIMELINE

- **Data Loading**: 5-10 minutes (‚úÖ Complete)
- **Epoch 1**: 15-20 minutes
- **Total Training**: 5-8 hours (with early stopping)
- **Expected Completion**: Tonight or tomorrow morning

## üéØ EXPECTED RESULTS

**Target Performance**:
- Val NRMSE: 0.20-0.28 (vs 0.3008 SAM baseline)
- Improvement: 0-25% better than SAM
- Combined C1+C2: 0.20-0.25 overall

**Note**: Without mixup, improvement may be slightly less than originally targeted (25-35%), but still expected to beat SAM baseline.

## üì¶ FILES CREATED/MODIFIED

### Created

1. `scripts/setup/setup_rocm_system.sh` - System-wide ROCm SDK setup
2. `.github/instructions/rocm_sdk_rule.instruction.md` - Memory rule
3. `ROCM_SDK_COMPLETE_SETUP.md` - This document

### Modified
1. `~/.bashrc` - Added ROCm SDK environment (permanent)
2. `scripts/launchers/start_c1_enhanced_training.sh` - Uses ROCm SDK, batch_size=4, mixup_alpha=0.0
3. `train_c1_enhanced.py` - num_workers=0 in DataLoader

## üöÄ USAGE FOR FUTURE TRAINING

All future training scripts will automatically use ROCm SDK because:
1. ‚úÖ Environment variables in ~/.bashrc (loaded at terminal start)
2. ‚úÖ Python aliases point to ROCm SDK
3. ‚úÖ Memory rule ensures correct configuration

**To train new models**:
```bash
# Just run python3 - it will use ROCm SDK automatically
python3 your_training_script.py

# Or explicitly:
/opt/rocm_sdk_612/bin/python3 your_training_script.py
```

## üí° LESSONS LEARNED

1. **Always use ROCm SDK Python** for AMD GPU training
2. **MNE needs special config** (offscreen rendering, no CUDA)
3. **DataLoader num_workers=0** prevents multiprocessing issues with MNE
4. **Batch size matters** for VRAM constraints (16‚Üí4 = 75% memory reduction)
5. **Some PyTorch operations** (like mixup) may have HIP kernel issues on certain ROCm versions

## ‚úÖ SUCCESS CRITERIA

- [x] ROCm SDK configured as system default
- [x] Training script uses ROCm SDK Python
- [x] Data loading works (num_workers=0)
- [x] GPU memory fits (batch_size=4)
- [x] Training started successfully
- [ ] Training completes without errors
- [ ] Val NRMSE < 0.28 achieved

---

**Status**: All setup complete, training in progress üöÄ
**Next**: Monitor training and upload SAM submission meanwhile
